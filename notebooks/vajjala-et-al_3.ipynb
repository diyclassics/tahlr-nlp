{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representation\n",
    "\n",
    "Code notebook for TAHLR Working Group (Spring 2024) based on:  \n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., and Surana, H. 2020. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. Sebastopol, CA: Oâ€™Reilly Media.\n",
    "\n",
    "More info on book here: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process docs\n",
    "\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary\n",
    "\n",
    "vocab = {}\n",
    "count = 0\n",
    "for doc in processed_docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count +1\n",
    "            vocab[word] = count\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one-hot encoded vector for a doc\n",
    "\n",
    "def get_onehot_vector(somestring):\n",
    "  onehot_encoded = []\n",
    "  for word in somestring.split():\n",
    "             temp = [0]*len(vocab)\n",
    "             if word in vocab:\n",
    "                        temp[vocab[word]-1] = 1\n",
    "             onehot_encoded.append(temp)\n",
    "  return onehot_encoded\n",
    "\n",
    "print(get_onehot_vector(processed_docs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one-hot representation for a random text, using the above vocabulary\n",
    "\n",
    "get_onehot_vector(\"man and dog are good\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "\n",
    "get_onehot_vector(\"man and man are good\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make count vectorizer with sci-kit learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer()\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = CV.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", sorted(CV.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the BOW rep for all documents with pandas\n",
    "\n",
    "df = pd.DataFrame(bow_rep.toarray(), columns = CV.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the representation using this vocabulary for a new text using transform\n",
    "\n",
    "temp = CV.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", \n",
    "\n",
    "temp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer with binarizer\n",
    "\n",
    "CVB = CountVectorizer(binary=True)\n",
    "bow_rep_bin = CVB.fit_transform(processed_docs)\n",
    "temp = CVB.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of ngrams\n",
    "\n",
    "#n-gram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "CVNG = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = CVNG.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", CVNG.vocabulary_)\n",
    "\n",
    "df = pd.DataFrame(bow_rep.toarray(), columns = CVNG.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the representation using this vocabulary for a new text\n",
    "\n",
    "temp = CVNG.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf-idf matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TV = TfidfVectorizer()\n",
    "bow_rep_tfidf = TV.fit_transform(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the idf values\n",
    "\n",
    "vocab = TV.get_feature_names_out()\n",
    "idfs = TV.idf_\n",
    "\n",
    "# Make dictionary\n",
    "\n",
    "word_idfs = dict(zip(vocab, idfs))\n",
    "pprint(word_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tf-idf DTM\n",
    "\n",
    "tv_df = pd.DataFrame(bow_rep_tfidf.toarray(), columns = TV.get_feature_names_out())\n",
    "tv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with Latin example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"Alienum est omne, quicquid optando evenit.\n",
    "Ab alio expectes, alteri quod feceris.\n",
    "Animus vereri qui scit, scit tuta ingredi.\n",
    "Auxilia humilia firma consensus facit.\n",
    "Amor animi arbitrio sumitur, non ponitur.\n",
    "Aut amat aut odit mulier, nil est tertium.\n",
    "Ad tristem partem strenua est suspicio.\n",
    "Ames parentem, si aequus est: si aliter, feras.\n",
    "Aspicere oportet, quidquid possis perdere.\n",
    "Amici vitia si feras, facias tua.\n",
    "Alienum aes homini ingenuo acerba est servitus.\n",
    "Absentem laedit, cum ebrio qui litigat.\n",
    "Amans iratus muta mentitur sibi.\n",
    "Avarus ipse miseriae causa est suae.\n",
    "Amans quid cupiat scit, quid sapiat non vidit.\n",
    "Amans quod suspicatur, vigilans somniat.\n",
    "Ad calamitatem quilibet rumor valet.\n",
    "Amor extorqueri non pote, elabi pote.\n",
    "Ab amante lacrimis redimas iracundiam.\n",
    "Aperte mala cum est mulier, tum demum est bona.\n",
    "Avarum facile capias, ubi non sis item.\n",
    "Amare et sapere vix deo conceditur.\n",
    "Avarus nisi cum moritur, nil recte facit.\n",
    "Astus cinaedum celat, aetas indicat.\n",
    "Avarus damno potius quam sapiens dolet.\n",
    "Avaro quid mali optes nisi Vivat diu?\n",
    "Animo dolenti nil oportet credere.\n",
    "Aliena nobis, nostra plus aliis placent.\n",
    "Amare iuveni fructus est, crimen seni.\n",
    "Anus cum ludit, morti delicias facit.\n",
    "Amoris vulnus idem, qui sanat, facit.\n",
    "Ad paenitendum properat cito qui iudicat.\n",
    "Aleator quanto in arte est melior, tanto est nequior.\n",
    "Amor otiosae causa sollicitudinis.\n",
    "Avidum esse oportet neminem, minime senem.\n",
    "Animo virum pudicae, non oculo eligunt.\n",
    "Amantium ira amoris integratio est.\n",
    "Amantis ius iurandum poenam non habet.\n",
    "Amans, sicut fax, agitando ardescit magis.\n",
    "Amor, ut lacrima, ab oculo oritur, in pectus cadit.\n",
    "Animo imperabit sapiens, stultus serviet.\n",
    "Amicum an nomen habeas, aperit calamitas.\n",
    "Amori finem tempus, non animus facit.\"\"\".split(\"\\n\")\n",
    "\n",
    "print(sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the sentences\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    import string\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('v', 'u').replace('j', 'i')\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "sents = [preprocess(sent) for sent in sents]\n",
    "\n",
    "print(sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make CV\n",
    "\n",
    "CV = CountVectorizer()\n",
    "dtm = CV.fit_transform(sents).toarray()\n",
    "vocab = CV.get_feature_names_out()\n",
    "cv_df = pd.DataFrame(dtm, columns = vocab)\n",
    "cv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf-idf\n",
    "\n",
    "TV = TfidfVectorizer()\n",
    "dtm = TV.fit_transform(sents).toarray()\n",
    "vocab = TV.get_feature_names_out()\n",
    "tv_df = pd.DataFrame(dtm, columns = vocab)\n",
    "tv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance ranking for Latin poem using line-based tf-idf dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all lines in Lucretius' De Rerum Natura\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "\n",
    "CR = LatinTesseraeCorpusReader()\n",
    "\n",
    "lucretius_files = CR.fileids(match='de_rerum_natura')\n",
    "lucretius_lines = list(CR.lines(fileids=lucretius_files))\n",
    "lucretius_cits = [line._.citation for line in lucretius_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show CLTK Readers line citations\n",
    "\n",
    "lucretius_cits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess lines\n",
    "\n",
    "lucretius_lines = [preprocess(line.text) for line in lucretius_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucretius_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFIDF matrix by line\n",
    "\n",
    "TV = TfidfVectorizer()\n",
    "dtm = TV.fit_transform(lucretius_lines)\n",
    "dtm = dtm.todense()\n",
    "vocab = TV.get_feature_names_out()\n",
    "df = pd.DataFrame(dtm, index=lucretius_cits, columns=vocab)\n",
    "df.iloc[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query vector; note `transform` method\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "query = \"hunc igitur terrorem animi tenebrasque necessest\"\n",
    "query = preprocess(query)\n",
    "query_vec = TV.transform([query]).todense()\n",
    "query_vec = np.array(query_vec).reshape(1, -1) # needs to be reshaped to match the shape of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity scores for every row against query vector, return 10 most similar\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_array = df.to_numpy()\n",
    "similarity_scores = cosine_similarity(query_vec, df_array)\n",
    "\n",
    "# top_ten_indices = np.argsort(similarity_scores[0])[::-1][:10]\n",
    "top_ten_indices = np.argsort(similarity_scores[0])[::-1][:15]\n",
    "\n",
    "print('Here are the top ten most similar lines to the query by TFIDF:')\n",
    "\n",
    "data = []\n",
    "for idx in top_ten_indices:\n",
    "    # print(f'{lucretius_cits[idx]}\\t{lucretius_lines[idx]}\\t{similarity_scores[0][idx]}')\n",
    "    data.append([lucretius_cits[idx], lucretius_lines[idx], similarity_scores[0][idx]])\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(data, headers=['Citation', 'Text', 'Similarity Score']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

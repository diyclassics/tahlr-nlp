{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "\n",
    "Code notebook for TAHLR Working Group (Spring 2024) based on:  \n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., and Surana, H. 2020. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. Sebastopol, CA: O‚ÄôReilly Media.\n",
    "\n",
    "More info on book here: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Installs\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -U https://huggingface.co/latincy/la_core_web_sm/resolve/main/la_core_web_sm-any-py3-none-any.whl\n",
    "!sudo apt install tesseract-ocr\n",
    "!pip install pytesseract\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Downloads\n",
    "\n",
    "import os\n",
    "# if not os.path.exists('somefile.png'):\n",
    "if True:\n",
    "    # Download the file\n",
    "    import urllib.request\n",
    "    url = 'https://www.dropbox.com/scl/fi/5en6qvay08hxa6lowg3y5/somefile.png?rlkey=kw1f3s87apostym9gva3amfk6&dl=1'\n",
    "    local_path = 'somefile.png'\n",
    "    urllib.request.urlretrieve(url, local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Parsing and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
    "\n",
    "# Set the User-Agent header\n",
    "req = urllib.request.Request(myurl, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "# Open the URL with the modified request\n",
    "html = urllib.request.urlopen(req).read()\n",
    "soupified = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(soupified.prettify()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question from post\n",
    "\n",
    "question = soupified.find(\"div\", {\"class\": \"question\"})\n",
    "questiontext = question.find(\"div\", {\"class\": \"s-prose\"}).find('p')\n",
    "print(f\"Question: {questiontext.get_text().strip()}\")\n",
    "\n",
    "answer = soupified.find(\"div\", {\"class\": \"answer\"})\n",
    "answertext = answer.find(\"div\", {\"class\": \"s-prose\"}).find('p')\n",
    "print(f\"Answer: {answertext.get_text().strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using Perseus\n",
    "\n",
    "# Get html\n",
    "url = \"https://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0133%3Abook%3D1%3Acard%3D1\"\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = urllib.request.urlopen(req).read()\n",
    "soupified = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Get Perseus text\n",
    "poem = soupified.find(\"div\", {\"class\": \"text_container\"})\n",
    "poemtext = poem.find_all(\"div\", {\"class\": \"text\"})\n",
    "\n",
    "for line in poemtext:\n",
    "    print(line.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I love Pizza üçï!  Shall we book a cab üöï to get pizza?'\n",
    "Text = text.encode(\"utf-8\")\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Text.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greek examples...\n",
    "\n",
    "print(\"Œº·øÜŒΩŒπŒΩ ·ºÑŒµŒπŒ¥Œµ Œ∏Œµ·Ω∞ Œ†Œ∑ŒªŒ∑œäŒ¨Œ¥Œµœâ ·ºàœáŒπŒª·øÜŒøœÇ\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b'\\xce\\xbc\\xe1\\xbf\\x86\\xce\\xbd\\xce\\xb9\\xce\\xbd \\xe1\\xbc\\x84\\xce\\xb5\\xce\\xb9\\xce\\xb4\\xce\\xb5 \\xce\\xb8\\xce\\xb5\\xe1\\xbd\\xb0 \\xce\\xa0\\xce\\xb7\\xce\\xbb\\xce\\xb7\\xcf\\x8a\\xce\\xac\\xce\\xb4\\xce\\xb5\\xcf\\x89 \\xe1\\xbc\\x88\\xcf\\x87\\xce\\xb9\\xce\\xbb\\xe1\\xbf\\x86\\xce\\xbf\\xcf\\x82'.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the following!\n",
    "\n",
    "eta_with_circumflex = \"·øÜ\".encode(\"utf-8\")\n",
    "print(eta_with_circumflex)\n",
    "print(len(eta_with_circumflex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_with_circumflex = \"·øÜ\".encode(\"utf-8\").decode(\"utf-8\")\n",
    "print(eta_with_circumflex)\n",
    "print(len(eta_with_circumflex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text from scanned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('somefile.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "filename = \"somefile.png\"\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence and word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "mytext = \"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. It is the series of steps involved in building any NLP model. These steps are common in every NLP project, so it makes sense to study them in this chapter. Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace. Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process. In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines about when and how to use which step. In later chapters, we‚Äôll discuss specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in my_sentences:\n",
    "   print(sentence)\n",
    "   print(word_tokenize(sentence))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "       return [token.lower() for token in tokens if token not in mystopwords and\n",
    "               not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latin example\n",
    "\n",
    "texts = [\"\"\"Tityre, tu patulae recubans sub tegmine fagi\n",
    "silvestrem tenui Musam meditaris avena;\n",
    "nos patriae fines et dulcia linquimus arva.\n",
    "nos patriam fugimus; tu, Tityre, lentus in umbra\n",
    "formosam resonare doces Amaryllida silvas.               5\"\"\"]\n",
    "\n",
    "print(preprocess_corpus(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "word1, word2 = \"cars\", \"revolution\"\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with spaCy\n",
    "\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "token = sp(u'better')\n",
    "for word in token:\n",
    "   print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with spaCy, Latin\n",
    "\n",
    "preprocessed_text = \" \".join(preprocess_corpus(texts)[0])\n",
    "\n",
    "nlp = spacy.load('la_core_web_sm')\n",
    "doc = nlp(preprocessed_text.split()[2])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(preprocessed_text)\n",
    "\n",
    "data = []\n",
    "\n",
    "for token in doc:\n",
    "    data.append((token.text, token.lemma_, token.pos_,\n",
    "          token.shape_, token.is_alpha, token.is_stop))\n",
    "    \n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(data, headers=[\"Token\", \"Lemma\", \"POS\", \"Shape\", \"Is Alpha\", \"Is Stop\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representation\n",
    "\n",
    "Code notebook for TAHLR Working Group (Spring 2024) based on:  \n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., and Surana, H. 2020. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. Sebastopol, CA: Oâ€™Reilly Media.\n",
    "\n",
    "More info on book here: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "!pip install -U git+https://github.com/diyclassics/cltk_readers.git#egg=cltk-readers\n",
    "from cltk.data.fetch import FetchCorpus\n",
    "corpus_downloader = FetchCorpus(language=\"lat\")\n",
    "corpus_downloader.import_corpus('lat_text_tesserae')\n",
    "corpus_downloader.import_corpus('lat_models_cltk')\n",
    "\n",
    "# Imports\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \"slim\" version of model\n",
    "\n",
    "model_url = \"https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "!curl -L $model_url -o models/GoogleNews-vectors-negative300-SLIM.bin.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "pretrainedpath = \"models/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of words in the vocabulary.\n",
    "\n",
    "print(f'There are {len(w2v_model.key_to_index)} words in the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show \"similar\" words\n",
    "print(\"Using this w2v model, the most similar words to 'beautiful' are:\")\n",
    "pprint(w2v_model.most_similar('beautiful')) # Note the parentheses, not square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the vector representation of a word\n",
    "\n",
    "w2v_model['beautiful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show shape of the vector\n",
    "\n",
    "w2v_model['beautiful'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import a test data set provided in gensim to train a model\n",
    "\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model, by selecting the parameters; save\n",
    "\n",
    "our_model = Word2Vec(common_texts, vector_size=10, window=5, min_count=1, workers=4)\n",
    "our_model.save(\"models/tempmodel.w2v\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(our_model.wv.most_similar('computer', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(our_model.wv['computer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Beyond Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sentence using the model\n",
    "doc = nlp(\"Canada is a large country and a neighbor to the north.\")\n",
    "\n",
    "#Get a vector for individual words\n",
    "print(doc[0].vector) #vector for 'Canada', the first word in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.vector) #Averaged vector for the entire sentence\n",
    "print(doc.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2].text == doc[6].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2].vector == doc[6].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the vectors with tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vectors for specific set of words\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get the vectors for the words in w2v_model\n",
    "words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"]\n",
    "words += ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew', 'kiwi', 'lemon']\n",
    "words += ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'black', 'white', 'gray']\n",
    "words += ['dog', 'cat', 'bird', 'fish', 'hamster', 'rabbit', 'turtle', 'lizard', 'snake', 'frog']\n",
    "\n",
    "vectors = [w2v_model[word] for word in words]\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the vectors to 2D\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=10)\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "x, y = zip(*vectors_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vectors\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a list of colors for each marker\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot the scatter plot with different colors for every 10 items\n",
    "for i in range(0, len(x), 10):\n",
    "    plt.scatter(x[i:i+10], y[i:i+10], c=colors[i//10])\n",
    "\n",
    "for i in range(len(words)):\n",
    "    plt.text(vectors_2d[i, 0], vectors_2d[i, 1], words[i])\n",
    "\n",
    "plt.title('t-SNE visualization of Word2Vec vectors for four categorical lists')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with Latin, pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LiLa lemma embeddings\n",
    "# cf. https://embeddings.lila-erc.eu/samples/download/word2vec/\n",
    "\n",
    "pretrained_latin_url = \"https://embeddings.lila-erc.eu/samples/download/word2vec/allLASLAlemmi-vector-100-nocase-w10-SKIP.vec\"\n",
    "!curl -L $pretrained_latin_url -o models/allLASLAlemmi-vector-100-nocase-w10-SKIP.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LiLa embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "pretrained_lila_path = \"models/allLASLAlemmi-vector-100-nocase-w10-SKIP.vec\"\n",
    "lila_model = KeyedVectors.load_word2vec_format(pretrained_lila_path, binary=False)\n",
    "print('done loading LiLa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lila_model.most_similar('oratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own Latin embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Latin w2v vectors\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "CR = LatinTesseraeCorpusReader()\n",
    "files = CR.fileids(match=\"cicero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make helper function\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('v', 'u')\n",
    "    text = text.replace('j', 'i')\n",
    "\n",
    "    from string import punctuation\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of sentences; NB: this takes several minutes (~7)\n",
    "\n",
    "sents = list(CR.sents(files))\n",
    "sents = [preprocess(\" \".join([token.lemma_ for token in sent])) for sent in sents ]\n",
    "sents = [sent.split() for sent in sents]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(sents, open(\"data/cicero_lemma_sents.pkl\", \"wb\"))\n",
    "sents = pickle.load(open(\"data/cicero_lemma_sents.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model as above for the Latin data\n",
    "\n",
    "our_model = Word2Vec(sents, vector_size=100, window=5, min_count=2, epochs=10, workers=4)\n",
    "our_model.save(\"models/temp-latmodel.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model.wv.most_similar('oratio', topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

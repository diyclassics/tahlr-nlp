{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Week 9d: Text Classification Algorithms\n",
    "\n",
    "Code notebook for TAHLR course at ISAW (Fall 2023) including a streamlined example of notebook 9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # installs\n",
    "# # uncomment and install if necessary\n",
    "\n",
    "# !python -m pip install lime\n",
    "# !python -m pip install git+https://github.com/diyclassics/cltk_readers.git#egg=cltkreaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "from cltkreaders.grc import GreekTesseraeCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils script for working with AG text\n",
    "\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Helper function for preprocessing\n",
    "def preprocess(\n",
    "    text,\n",
    "    lower=True,\n",
    "    normalize=True,\n",
    "    punctuation=False,\n",
    "    numbers=False,\n",
    "    unhyphenate=False,\n",
    "    remove_lines=False,\n",
    "    remove_spaces=False,\n",
    "    entities=False,\n",
    "    diacriticals=True,\n",
    "    fill=\" \",\n",
    "):\n",
    "    if not entities:\n",
    "        text = html.unescape(text)\n",
    "\n",
    "    if unhyphenate:\n",
    "        text = re.sub(r\"[-»—]\\s?\\n\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    if lower:\n",
    "        text = text.lower()  # Lowercase\n",
    "\n",
    "    if not punctuation:\n",
    "        # Remove punctuation\n",
    "        punctuation = \"\\\"#$%&'()*+,/:;<=>@[\\]^_`{|}~.?!«»—“-”\"\n",
    "        misc = \"¡£¤¥¦§¨©¯°±²³´µ¶·¸¹º¼½¾¿÷·–‘’†•ↄ∞⏑〈〉（）\"\n",
    "        misc += punctuation\n",
    "        translator = str.maketrans({key: fill for key in misc})\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if not numbers:\n",
    "        # Remove numbers\n",
    "        translator = str.maketrans({key: fill for key in \"0123456789\"})\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if remove_lines:\n",
    "        text = \" \".join(text.split(\"\\n\"))\n",
    "\n",
    "    if remove_spaces:\n",
    "        text = fill.join(text.split())\n",
    "\n",
    "    if not diacriticals:\n",
    "        # text = remove_diacriticals(text)\n",
    "        pass\n",
    "\n",
    "    # Fix spacing\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corpus readers/files\n",
    "\n",
    "GCR = GreekTesseraeCorpusReader()\n",
    "\n",
    "plato = GCR.fileids(match='plato')\n",
    "aristotle = GCR.fileids(match='aristotle')\n",
    "herodotus = GCR.fileids(match='herodotus')\n",
    "thucydides = GCR.fileids(match='thucydides')\n",
    "\n",
    "plato_sents = list(GCR.sents(plato))\n",
    "aristotle_sents = list(GCR.sents(aristotle))\n",
    "herodotus_sents = list(GCR.sents(herodotus))\n",
    "thucydides_sents = list(GCR.sents(thucydides))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess texts\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    import unicodedata\n",
    "    def strip_diacritics(text):\n",
    "        text = preprocess(text)\n",
    "        # strip diacritics from greek words with function\n",
    "        stripped_text = ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "        return unicodedata.normalize('NFC', stripped_text)\n",
    "    return strip_diacritics(text)\n",
    "\n",
    "plato_sents = [custom_preprocess(sent) for sent in plato_sents][:5000]\n",
    "aristotle_sents = [custom_preprocess(sent) for sent in aristotle_sents][:5000]\n",
    "herodotus_sents = [custom_preprocess(sent) for sent in herodotus_sents]\n",
    "thucydides_sents = [custom_preprocess(sent) for sent in thucydides_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe\n",
    "\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill dataframe\n",
    "\n",
    "class_names = ['philosopy', 'historiography']\n",
    "df['class'] = [0 for sent in plato_sents] + [0 for sent in aristotle_sents] + [1 for sent in herodotus_sents] + [1 for sent in thucydides_sents]    \n",
    "df['texts'] = plato_sents + aristotle_sents + herodotus_sents + thucydides_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary info for class\n",
    "\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stops\n",
    "\n",
    "all_words = [word for sent in df['texts'] for word in sent.split()]\n",
    "\n",
    "from collections import Counter\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "STOPWORDS = [word for word, count in word_counts.most_common(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/test splits\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['texts'],\n",
    "                                                    df['class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['class'])\n",
    "\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=STOPWORDS, max_features=10000)),\n",
    "    ('svm', SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy\n",
    "\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make confusion matrix\n",
    "\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make confusion matrix\n",
    "# nb: plot_confusion_matrix as shown in *Blueprints* is deprecated; use ConfusionMatrixDisplay instead as shown below [PJB 11.3.2023]\n",
    "\n",
    "CMD = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "CMD.plot(cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create explainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change X_test to a list \n",
    "\n",
    "X_test_list = X_test.tolist()\n",
    "Y_test_list = Y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function for \"verbose\" explanation\n",
    "\n",
    "def generate_explanation(idx, class_names=class_names):\n",
    "    exp = explainer.explain_instance(X_test_list[idx], pipeline.predict_proba, num_features = 5)\n",
    "    print(f'Document id: {idx}')\n",
    "    print(f'Probability (0 = {class_names[0]}, 1 = {class_names[1]}) =', pipeline.predict_proba([X_test_list[idx]])[0,1])\n",
    "    print(f'True class: {Y_test.iloc[idx]} ({class_names[0] if Y_test_list[idx] == 0 else class_names[1]})')\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an explanation example\n",
    "\n",
    "idx = 6\n",
    "exp = generate_explanation(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show explanation as list\n",
    "\n",
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show explanation as barplot\n",
    "\n",
    "fig = exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show explanation as annotated text\n",
    "\n",
    "idx = 6\n",
    "exp = generate_explanation(idx)\n",
    "exp.show_in_notebook(text = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random explanation from test set\n",
    "\n",
    "import random\n",
    "\n",
    "for idx in random.sample(range(len(X_test)), 5):\n",
    "    exp = generate_explanation(idx)\n",
    "    exp.show_in_notebook(text = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "janWv1vG5xUD"
   },
   "source": [
    "# Text Classification with Naive Bayes, Logistic Regression, SVM\n",
    "\n",
    "Code notebook for TAHLR Working Group (Spring 2024) based on:  \n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., and Surana, H. 2020. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. Sebastopol, CA: Oâ€™Reilly Media.\n",
    "\n",
    "More info on book here: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBCjEALX5xWj"
   },
   "source": [
    "**Overview:** This notebook aims to give you a brief overview of performing text classification using Naive Bayes, Logistic Regression and Support Vector Machines. We will be using a dataset called \"Economic news article tone and relevance\" from [Figure-Eight](https://github.com/practical-nlp/practical-nlp/blob/master/Ch4/Data/Full-Economic-News-DFE-839861.csv) which consists of approximately 8000 news articles, which were tagged as relevant or not relevant to the US Economy. Our goal in this notebook is to explore the process of training and testing text classifiers for this problem, using this data set and two text classification algorithms: Multinomial Naive Bayes and Logistic Regression, implemented in sklearn. \n",
    "\n",
    "##### Dataset Link: In the a folder called Data in folder Ch4 of this repo\n",
    "<br><br>\n",
    "Let's import few necessary packages before we start our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBvvarqE5xWm"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd # to work with csv files\n",
    "\n",
    "# matplotlib imports are used to plot confusion matrices for the classifiers\n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# import feature extraction methods from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "\n",
    "# pre-processing of text\n",
    "import string\n",
    "import re\n",
    "\n",
    "# import classifiers from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import different metrics to evaluate the classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import metrics\n",
    "\n",
    "# import time function from time module to track the training duration\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1giNRemr1lk7"
   },
   "source": [
    "### Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVD8N_E51lk7",
    "outputId": "b5893f5e-1123-43f7-d3a5-2e4fb92bfdc9"
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "\n",
    "!mkdir -p data/\n",
    "data_url = \"https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Full-Economic-News-DFE-839861.csv\"\n",
    "!curl -L $data_url -o data/Full-Economic-News-DFE-839861.csv\n",
    "\n",
    "our_data = pd.read_csv(\"data/Full-Economic-News-DFE-839861.csv\" , encoding = \"ISO-8859-1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "LbED8Q185xWu",
    "outputId": "2ded8ddf-5553-4f4a-b55f-16454270648d"
   },
   "outputs": [],
   "source": [
    "display(our_data.shape) # Number of rows (instances) and columns in the dataset\n",
    "our_data[\"relevance\"].value_counts()/our_data.shape[0] # Class distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYW_S3585xXF",
    "outputId": "b64bb281-6512-43b5-eda9-73d43becb1ae"
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "our_data = our_data[our_data.relevance != \"not sure\"] # removing the data where we don't want relevance=\"not sure\".\n",
    "our_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_data['relevance'] = our_data.relevance.map({'yes':1, 'no':0}) # relevant is 1, not-relevant is 0. \n",
    "our_data = our_data[['text', 'relevance']] # Let us take only the two columns we need.\n",
    "our_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOKz8xQr5xXJ"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MZSHdHZ5xXL"
   },
   "outputs": [],
   "source": [
    "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "def clean(doc): # doc is a string of text\n",
    "    doc = doc.replace(\"</br>\", \" \") # This text contains a lot of <br/> tags.\n",
    "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
    "    doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
    "    # remove punctuation and numbers\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CfVm42o5xXS"
   },
   "source": [
    "### Modeling\n",
    "\n",
    "Steps:\n",
    "1 Split the data into training and test sets (75% train, 25% test)    \n",
    "2 Extract features from the training data using CountVectorizer, which is a bag of words feature  implementation. We will use the pre-processing function above in conjunction with Count Vectorizer  \n",
    "3 Transform the test data into the same feature vector as the training data.  \n",
    "4 Train the classifier  \n",
    "5 Evaluate the classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GimJJHhg5xYl",
    "outputId": "7ed9cad8-3bd8-416d-a352-4a44fad9dc80"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: train-test split\n",
    "X = our_data.text # the column text contains textual data to extract features from\n",
    "y = our_data.relevance # this is the column we are learning to predict. \n",
    "print(X.shape, y.shape)\n",
    "# split X and y into training and testing sets. By default, it splits 75% training and 25% test\n",
    "# random_state=1 for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsUyIBUD5xZI",
    "outputId": "f4082e6a-a1e9-4b4a-c247-8b1b84c7edae"
   },
   "outputs": [],
   "source": [
    "# Step 2-3: Preprocess and Vectorize train and test data\n",
    "vect = CountVectorizer(preprocessor=clean) # instantiate a vectoriezer\n",
    "X_train_dtm = vect.fit_transform(X_train)# use it to extract features from training data\n",
    "# transform testing data (using training data's features)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "# i.e., the dimension of our feature vector is 49753!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDLwA4CL5xZq",
    "outputId": "3cb119d8-3017-4ebb-89b9-86dca66e3e92"
   },
   "outputs": [],
   "source": [
    "# Step 3: Train the classifier and predict for test data\n",
    "nb = MultinomialNB() # instantiate a Multinomial Naive Bayes model\n",
    "%time nb.fit(X_train_dtm, y_train) # train the model(timing it with an IPython \"magic command\")\n",
    "y_pred_class = nb.predict(X_test_dtm) # make class predictions for X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "LiCHjvc75xZ3",
    "outputId": "1409e48f-0ed6-4705-8688-4e6126662863"
   },
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the classifier using various measures\n",
    "\n",
    "# Function to plot confusion matrix. \n",
    "# Ref:http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)\n",
    "    \n",
    "    \n",
    "# Print accuracy:\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "    \n",
    "# print the confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Not Relevant','Relevant'],normalize=True,\n",
    "                      title='Confusion matrix with all features')\n",
    "\n",
    "# calculate AUC: Area under the curve(AUC) gives idea about the model efficiency:\n",
    "# Further information: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "print(\"ROC_AOC_Score: \", roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "ylOI4OsD5xaE",
    "outputId": "0aea4279-84d2-49d3-e979-30e7c911f814"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(preprocessor=clean, max_features=5000) # Step-1\n",
    "X_train_dtm = vect.fit_transform(X_train) # combined step 2 and 3\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "nb = MultinomialNB() # instantiate a Multinomial Naive Bayes model\n",
    "%time nb.fit(X_train_dtm, y_train) # train the model(timing it with an IPython \"magic command\")\n",
    "y_pred_class = nb.predict(X_test_dtm) # make class predictions for X_test_dtm\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\n",
    "# print the confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Not Relevant','Relevant'],normalize=True,\n",
    "                      title='Confusion matrix with max 5000 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "0v7pM9hB5xbA",
    "outputId": "292bdf0c-924b-494b-ffae-c4914f2f5db9"
   },
   "outputs": [],
   "source": [
    "# Change classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # import\n",
    "\n",
    "logreg = LogisticRegression(class_weight=\"balanced\") # instantiate a logistic regression model\n",
    "logreg.fit(X_train_dtm, y_train) # fit the model with training data\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "\n",
    "# calculate evaluation measures:\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_class))\n",
    "print(\"AUC: \", roc_auc_score(y_test, y_pred_prob))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Not Relevant','Relevant'],normalize=True,\n",
    "                      title='Confusion matrix with normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "XJLKusAQ5xbf",
    "outputId": "4dcdc0d5-4f4f-487a-ac44-2bc6778a0876"
   },
   "outputs": [],
   "source": [
    "# Change classifier, number of features\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "vect = CountVectorizer(preprocessor=clean, max_features=1000) # Step-1\n",
    "X_train_dtm = vect.fit_transform(X_train) # combined step 2 and 3\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "classifier = LinearSVC(class_weight='balanced') # instantiate a logistic regression model\n",
    "classifier.fit(X_train_dtm, y_train) # fit the model with training data\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_class = classifier.predict(X_test_dtm)\n",
    "\n",
    "# calculate evaluation measures:\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_class))\n",
    "print(\"AUC: \", roc_auc_score(y_test, y_pred_prob))\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Not Relevant','Relevant'],normalize=True,\n",
    "                      title='Confusion matrix with normalization')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_OnePipeline_ManyClassifiers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

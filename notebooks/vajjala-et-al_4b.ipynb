{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "janWv1vG5xUD"
   },
   "source": [
    "# Word2Vec for Text Classification\n",
    "\n",
    "Code notebook for TAHLR Working Group (Spring 2024) based on:  \n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., and Surana, H. 2020. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. Sebastopol, CA: Oâ€™Reilly Media.\n",
    "\n",
    "More info on book here: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBCjEALX5xWj"
   },
   "source": [
    "**Overview:** In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "\n",
    "# !pip install fasttext-wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBvvarqE5xWm"
   },
   "outputs": [],
   "source": [
    "#basic imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "\n",
    "#pre-processing imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#imports related to modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1giNRemr1lk7"
   },
   "source": [
    "### Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVD8N_E51lk7",
    "outputId": "b5893f5e-1123-43f7-d3a5-2e4fb92bfdc9"
   },
   "outputs": [],
   "source": [
    "# Get files\n",
    "\n",
    "path = 'data/4b'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "files = [\"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch4/Data/sentiment%20labelled%20sentences/amazon_cells_labelled.txt\", \"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch4/Data/sentiment%20labelled%20sentences/imdb_labelled.txt\", \"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch4/Data/sentiment%20labelled%20sentences/yelp_labelled.txt\"]\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    urllib.request.urlretrieve(file, f\"{path}/{file_name}\")\n",
    "\n",
    "!cat data/4b/amazon_cells_labelled.txt data/4b/imdb_labelled.txt data/4b/yelp_labelled.txt > data/4b/sentiment_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "# Download \"slim\" version of model\n",
    "\n",
    "!mkdir -p models\n",
    "\n",
    "model_url = \"https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "\n",
    "if not os.path.exists('models/GoogleNews-vectors-negative300-SLIM.bin.gz'):\n",
    "    !curl -L $model_url -o models/GoogleNews-vectors-negative300-SLIM.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "pretrainedpath = \"models/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the model\n",
    "\n",
    "word2vec_vocab = w2v_model.key_to_index\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "#the file path consists of tab separated sentences and cats.\n",
    "\n",
    "training_data_path = \"data/4b/sentiment_sentences.txt\"\n",
    "\n",
    "texts = []\n",
    "cats = []\n",
    "\n",
    "with open(training_data_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        text, sentiment = line.split('\\t')\n",
    "        texts.append(text.strip())\n",
    "        cats.append(int(sentiment.strip()))\n",
    "\n",
    "cats = np.array(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the dataset\n",
    "\n",
    "print(len(cats), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "LbED8Q185xWu",
    "outputId": "2ded8ddf-5553-4f4a-b55f-16454270648d"
   },
   "outputs": [],
   "source": [
    "#preprocess the text.\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n",
    "               and token not in punctuation]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "texts_processed = preprocess_corpus(texts)\n",
    "print(len(cats), len(texts_processed))\n",
    "print(texts_processed[1])\n",
    "print(cats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create w2v representations of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYW_S3585xXF",
    "outputId": "b64bb281-6512-43b5-eda9-73d43becb1ae"
   },
   "outputs": [],
   "source": [
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this =  np.zeros(DIMENSION)\n",
    "        count_for_this = 0 + 1e-5 # to avoid divide-by-zero \n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "        if(count_for_this!=0):\n",
    "            feats.append(feat_for_this/count_for_this) \n",
    "        else:\n",
    "            feats.append(zero_vector)\n",
    "    return feats\n",
    "\n",
    "# # Refactor from book\n",
    "# def embedding_feats(list_of_lists):\n",
    "#     feats = []\n",
    "#     for tokens in list_of_lists:\n",
    "#         if tokens:\n",
    "#             feat_for_this= np.mean([w2v_model[token] for token in tokens if token in w2v_model], axis=0)\n",
    "#         else:\n",
    "#             feat_for_this = np.zeros(300)\n",
    "#         feats.append(feat_for_this)\n",
    "#     return feats\n",
    "\n",
    "train_vectors = embedding_feats(texts_processed)\n",
    "print(len(train_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
    "classifier.fit(train_data, train_cats)\n",
    "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
    "preds = classifier.predict(test_data)\n",
    "print(classification_report(test_cats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and mode (fasttext experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files\n",
    "\n",
    "files = [\"https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz\"]\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    urllib.request.urlretrieve(file, f\"{path}/{file_name}\")\n",
    "\n",
    "!tar -xvf data/4b/dbpedia_csv.tar.gz -C data/4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "data_path = 'data/4b'\n",
    "\n",
    "# Loading train data\n",
    "train_file = data_path + '/dbpedia_csv/train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "# Loading test data\n",
    "test_file = data_path + '/dbpedia_csv/test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "# Data we have\n",
    "print(\"Train:{} Test:{}\".format(df.shape,df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map classes\n",
    "\n",
    "# Since we have no clue about the classes lets build one\n",
    "# Mapping from class number to class name\n",
    "class_dict={\n",
    "            1:'Company',\n",
    "            2:'EducationalInstitution',\n",
    "            3:'Artist',\n",
    "            4:'Athlete',\n",
    "            5:'OfficeHolder',\n",
    "            6:'MeanOfTransportation',\n",
    "            7:'Building',\n",
    "            8:'NaturalPlace',\n",
    "            9:'Village',\n",
    "            10:'Animal',\n",
    "            11:'Plant',\n",
    "            12:'Album',\n",
    "            13:'Film',\n",
    "            14:'WrittenWork'\n",
    "        }\n",
    "\n",
    "# Mapping the classes\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data\n",
    "\n",
    "df[\"class_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some cleaning of this text\n",
    "def clean_it(text,normalize=True):\n",
    "    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain\n",
    "    s = str(text).replace(',',' ').replace('\"','').replace('\\'',' \\' ').replace('.',' . ').replace('(',' ( ').\\\n",
    "            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()\n",
    "    \n",
    "    # normalizing / encoding the text\n",
    "    if normalize:\n",
    "        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Now lets define a small function where we can use above cleaning on datasets\n",
    "def clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):\n",
    "    # Defining the new data\n",
    "    df = data[['name','description']].copy(deep=True)\n",
    "    df['class'] = label_prefix + data['class'].astype(str) + ' '\n",
    "    \n",
    "    # cleaning it\n",
    "    if cleanit:\n",
    "        df['name'] = df['name'].apply(lambda x: clean_it(x,encodeit))\n",
    "        df['description'] = df['description'].apply(lambda x: clean_it(x,encodeit))\n",
    "    \n",
    "    # shuffling it\n",
    "    if shuffleit:\n",
    "        df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Transform the datasets using the above clean functions\n",
    "df_train_cleaned = clean_df(df, True, True)\n",
    "df_test_cleaned = clean_df(df_test, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write files to disk as fastText classifier API reads files from disk.\n",
    "train_file = data_path + '/dbpedia_train.csv'\n",
    "df_train_cleaned.to_csv(train_file, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file = data_path + '/dbpedia_test.csv'\n",
    "df_test_cleaned.to_csv(test_file, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier using fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea of subword vectors\n",
    "\n",
    "def generate_char_ngrams(word, n):\n",
    "    # Adding start and end markers\n",
    "    padded_word = '<' + word + '>'\n",
    "    ngrams = [padded_word[i:i+n] for i in range(len(padded_word)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "# Example usage\n",
    "word = \"amaverunt\"\n",
    "n = 3\n",
    "ngrams = generate_char_ngrams(word, n)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## Using fastText for feature extraction and training\n",
    "# from fasttext import train_supervised \n",
    "# \"\"\"fastText expects and training file (csv), a model name as input arguments.\n",
    "# label_prefix refers to the prefix before label string in the dataset.\n",
    "# default is __label__. In our dataset, it is __class__. \n",
    "# There are several other parameters which can be seen in: \n",
    "# https://pypi.org/project/fasttext/\n",
    "# \"\"\"\n",
    "# model = train_supervised(input=train_file, label=\"__class__\", lr=1.0, epoch=75, loss='ova', wordNgrams=2, dim=200, thread=2, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(1,6):\n",
    "#     results = model.test(test_file,k=k)\n",
    "#     print(f\"Test Samples: {results[0]} Precision@{k} : {results[1]*100:2.4f} Recall@{k} : {results[2]*100:2.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained FastText vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import gensim\n",
    "\n",
    "# Download the pre-trained FastText vectors for Latin\n",
    "# You can find other languages here: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "latin_vectors_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.vec.gz'\n",
    "latin_vectors_path = 'models/cc.la.300.vec.gz'\n",
    "\n",
    "# Download the vectors (if not already downloaded)\n",
    "import requests\n",
    "if not os.path.exists(latin_vectors_path):\n",
    "    with requests.get(latin_vectors_url, stream=True) as r:\n",
    "        with open(latin_vectors_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "# Load the vectors using Gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(latin_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find nearest neighbors\n",
    "def get_nearest_neighbors(word, model, top_n=10):\n",
    "    try:\n",
    "        neighbors = model.most_similar(word, topn=top_n)\n",
    "        return neighbors\n",
    "    except KeyError:\n",
    "        return f\"The word '{word}' is not in the vocabulary.\"\n",
    "\n",
    "# Example usage\n",
    "word = 'pater'  # Replace with any Latin word\n",
    "neighbors = get_nearest_neighbors(word, model)\n",
    "\n",
    "print(f\"Nearest neighbors for '{word}':\")\n",
    "for neighbor, similarity in neighbors:\n",
    "    print(f\"{neighbor}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the FastText vectors for Latin if not already downloaded\n",
    "latin_model_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.bin.gz'\n",
    "latin_model_path = 'models/cc.la.300.bin.gz'\n",
    "uncompressed_model_path = 'models/cc.la.300.bin'\n",
    "\n",
    "# do the download with urllib\n",
    "\n",
    "if not os.path.exists(latin_model_path):\n",
    "    urllib.request.urlretrieve(latin_model_url, latin_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the model if not already done\n",
    "if not os.path.exists(uncompressed_model_path):\n",
    "    import gzip\n",
    "    with gzip.open(latin_model_path, 'rb') as f_in:\n",
    "        with open(uncompressed_model_path, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = fasttext.load_model(uncompressed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get subword vectors\n",
    "def get_subword_vectors(word, model):\n",
    "    subwords, indices = model.get_subwords(word)\n",
    "    subword_vectors = model.get_input_matrix()[indices]\n",
    "    return subwords, subword_vectors\n",
    "\n",
    "# Example usage\n",
    "word = 'amaverunt'\n",
    "subwords, subword_vectors = get_subword_vectors(word, model)\n",
    "\n",
    "print(f\"Subwords for '{word}':\")\n",
    "for subword in subwords:\n",
    "    print(subword)\n",
    "\n",
    "print(f\"\\nSubword vectors for '{word}':\")\n",
    "for i, vec in enumerate(subword_vectors):\n",
    "    print(f\"Subword: {subwords[i]} -> Vector: {vec[:10]}...\")  # Printing first 10 dimensions for brevity"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_OnePipeline_ManyClassifiers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

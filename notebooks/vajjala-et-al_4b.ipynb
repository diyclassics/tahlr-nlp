{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "janWv1vG5xUD"
   },
   "source": [
    "# Word2Vec for Text Classification\n",
    "\n",
    "Code notebook for TAHLR Working Group (Spring 2024) based on:  \n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., and Surana, H. 2020. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. Sebastopol, CA: O’Reilly Media.\n",
    "\n",
    "More info on book here: https://www.oreilly.com/library/view/practical-natural-language/9781492054047/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBCjEALX5xWj"
   },
   "source": [
    "**Overview:** In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "\n",
    "# !pip install numpy==1.19.5\n",
    "# !pip install pandas==1.1.5\n",
    "# !pip install gensim==3.8.3\n",
    "# !pip install wget==3.2\n",
    "# !pip install nltk==3.5\n",
    "# !pip install scikit-learn==0.21.3\n",
    "# !pip install fasttext-wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QBvvarqE5xWm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pjb311/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/pjb311/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#basic imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "\n",
    "#pre-processing imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#imports related to modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1giNRemr1lk7"
   },
   "source": [
    "### Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVD8N_E51lk7",
    "outputId": "b5893f5e-1123-43f7-d3a5-2e4fb92bfdc9"
   },
   "outputs": [],
   "source": [
    "# Get files\n",
    "\n",
    "path = 'data/4b'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "files = [\"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch4/Data/sentiment%20labelled%20sentences/amazon_cells_labelled.txt\", \"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch4/Data/sentiment%20labelled%20sentences/imdb_labelled.txt\", \"https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch4/Data/sentiment%20labelled%20sentences/yelp_labelled.txt\"]\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    urllib.request.urlretrieve(file, f\"{path}/{file_name}\")\n",
    "\n",
    "!cat data/4b/amazon_cells_labelled.txt data/4b/imdb_labelled.txt data/4b/yelp_labelled.txt > data/4b/sentiment_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  263M  100  263M    0     0  6866k      0  0:00:39  0:00:39 --:--:-- 6731k 263M   63  166M    0     0  6082k      0  0:00:44  0:00:27  0:00:17 10.1M6  176M    0     0  6226k      0  0:00:43  0:00:28  0:00:15 10.2M0  6791k      0  0:00:39  0:00:36  0:00:03 8155k\n"
     ]
    }
   ],
   "source": [
    "# Get model\n",
    "# Download \"slim\" version of model\n",
    "\n",
    "model_url = \"https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "\n",
    "if not os.path.exists('models/GoogleNews-vectors-negative300-SLIM.bin.gz'):\n",
    "    !curl -L $model_url -o models/GoogleNews-vectors-negative300-SLIM.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "pretrainedpath = \"models/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299567\n"
     ]
    }
   ],
   "source": [
    "#Inspect the model\n",
    "\n",
    "word2vec_vocab = w2v_model.key_to_index\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "#the file path consists of tab separated sentences and cats.\n",
    "\n",
    "training_data_path = \"data/4b/sentiment_sentences.txt\"\n",
    "\n",
    "texts = []\n",
    "cats = []\n",
    "\n",
    "with open(training_data_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        text, sentiment = line.split('\\t')\n",
    "        texts.append(text.strip())\n",
    "        cats.append(int(sentiment.strip()))\n",
    "\n",
    "cats = np.array(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n"
     ]
    }
   ],
   "source": [
    "#Inspect the dataset\n",
    "\n",
    "print(len(cats), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " 'Good case, Excellent value.',\n",
       " 'Great for the jawbone.',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       " 'The mic is great.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "LbED8Q185xWu",
    "outputId": "2ded8ddf-5553-4f4a-b55f-16454270648d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n",
      "['good', 'case', 'excellent', 'value']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#preprocess the text.\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n",
    "               and token not in punctuation]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "texts_processed = preprocess_corpus(texts)\n",
    "print(len(cats), len(texts_processed))\n",
    "print(texts_processed[1])\n",
    "print(cats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create w2v representations of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYW_S3585xXF",
    "outputId": "b64bb281-6512-43b5-eda9-73d43becb1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this =  np.zeros(DIMENSION)\n",
    "        count_for_this = 0 + 1e-5 # to avoid divide-by-zero \n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "        if(count_for_this!=0):\n",
    "            feats.append(feat_for_this/count_for_this) \n",
    "        else:\n",
    "            feats.append(zero_vector)\n",
    "    return feats\n",
    "\n",
    "# # Refactor from book\n",
    "# def embedding_feats(list_of_lists):\n",
    "#     feats = []\n",
    "#     for tokens in list_of_lists:\n",
    "#         if tokens:\n",
    "#             feat_for_this= np.mean([w2v_model[token] for token in tokens if token in w2v_model], axis=0)\n",
    "#         else:\n",
    "#             feat_for_this = np.zeros(300)\n",
    "#         feats.append(feat_for_this)\n",
    "#     return feats\n",
    "\n",
    "train_vectors = embedding_feats(texts_processed)\n",
    "print(len(train_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8186666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82       371\n",
      "           1       0.82      0.83      0.82       379\n",
      "\n",
      "    accuracy                           0.82       750\n",
      "   macro avg       0.82      0.82      0.82       750\n",
      "weighted avg       0.82      0.82      0.82       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
    "classifier.fit(train_data, train_cats)\n",
    "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
    "preds = classifier.predict(test_data)\n",
    "print(classification_report(test_cats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and mode (fasttext experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x dbpedia_csv/\n",
      "x dbpedia_csv/test.csv\n",
      "x dbpedia_csv/classes.txt\n",
      "x dbpedia_csv/train.csv\n",
      "x dbpedia_csv/readme.txt\n"
     ]
    }
   ],
   "source": [
    "# Get files\n",
    "\n",
    "files = [\"https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz\"]\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    urllib.request.urlretrieve(file, f\"{path}/{file_name}\")\n",
    "\n",
    "!tar -xvf data/4b/dbpedia_csv.tar.gz -C data/4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:(560000, 3) Test:(70000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "data_path = 'data/4b'\n",
    "\n",
    "# Loading train data\n",
    "train_file = data_path + '/dbpedia_csv/train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "# Loading test data\n",
    "test_file = data_path + '/dbpedia_csv/test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "# Data we have\n",
    "print(\"Train:{} Test:{}\".format(df.shape,df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                               name  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                         description class_name  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...    Company  \n",
       "1   Schwan-STABILO is a German maker of pens for ...    Company  \n",
       "2   Q-workshop is a Polish company located in Poz...    Company  \n",
       "3   Marvell Software Solutions Israel known as RA...    Company  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...    Company  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map classes\n",
    "\n",
    "# Since we have no clue about the classes lets build one\n",
    "# Mapping from class number to class name\n",
    "class_dict={\n",
    "            1:'Company',\n",
    "            2:'EducationalInstitution',\n",
    "            3:'Artist',\n",
    "            4:'Athlete',\n",
    "            5:'OfficeHolder',\n",
    "            6:'MeanOfTransportation',\n",
    "            7:'Building',\n",
    "            8:'NaturalPlace',\n",
    "            9:'Village',\n",
    "            10:'Animal',\n",
    "            11:'Plant',\n",
    "            12:'Album',\n",
    "            13:'Film',\n",
    "            14:'WrittenWork'\n",
    "        }\n",
    "\n",
    "# Mapping the classes\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_name\n",
       "Company                   40000\n",
       "EducationalInstitution    40000\n",
       "Artist                    40000\n",
       "Athlete                   40000\n",
       "OfficeHolder              40000\n",
       "MeanOfTransportation      40000\n",
       "Building                  40000\n",
       "NaturalPlace              40000\n",
       "Village                   40000\n",
       "Animal                    40000\n",
       "Plant                     40000\n",
       "Album                     40000\n",
       "Film                      40000\n",
       "WrittenWork               40000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data\n",
    "\n",
    "df[\"class_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some cleaning of this text\n",
    "def clean_it(text,normalize=True):\n",
    "    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain\n",
    "    s = str(text).replace(',',' ').replace('\"','').replace('\\'',' \\' ').replace('.',' . ').replace('(',' ( ').\\\n",
    "            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()\n",
    "    \n",
    "    # normalizing / encoding the text\n",
    "    if normalize:\n",
    "        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Now lets define a small function where we can use above cleaning on datasets\n",
    "def clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):\n",
    "    # Defining the new data\n",
    "    df = data[['name','description']].copy(deep=True)\n",
    "    df['class'] = label_prefix + data['class'].astype(str) + ' '\n",
    "    \n",
    "    # cleaning it\n",
    "    if cleanit:\n",
    "        df['name'] = df['name'].apply(lambda x: clean_it(x,encodeit))\n",
    "        df['description'] = df['description'].apply(lambda x: clean_it(x,encodeit))\n",
    "    \n",
    "    # shuffling it\n",
    "    if shuffleit:\n",
    "        df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Transform the datasets using the above clean functions\n",
    "df_train_cleaned = clean_df(df, True, True)\n",
    "df_test_cleaned = clean_df(df_test, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write files to disk as fastText classifier API reads files from disk.\n",
    "train_file = data_path + '/dbpedia_train.csv'\n",
    "df_train_cleaned.to_csv(train_file, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file = data_path + '/dbpedia_test.csv'\n",
    "df_test_cleaned.to_csv(test_file, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier using fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<am', 'ama', 'mav', 'ave', 'ver', 'eru', 'run', 'unt', 'nt>']\n"
     ]
    }
   ],
   "source": [
    "# Idea of subword vectors\n",
    "\n",
    "def generate_char_ngrams(word, n):\n",
    "    # Adding start and end markers\n",
    "    padded_word = '<' + word + '>'\n",
    "    ngrams = [padded_word[i:i+n] for i in range(len(padded_word)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "# Example usage\n",
    "word = \"amaverunt\"\n",
    "n = 3\n",
    "ngrams = generate_char_ngrams(word, n)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## Using fastText for feature extraction and training\n",
    "# from fasttext import train_supervised \n",
    "# \"\"\"fastText expects and training file (csv), a model name as input arguments.\n",
    "# label_prefix refers to the prefix before label string in the dataset.\n",
    "# default is __label__. In our dataset, it is __class__. \n",
    "# There are several other parameters which can be seen in: \n",
    "# https://pypi.org/project/fasttext/\n",
    "# \"\"\"\n",
    "# model = train_supervised(input=train_file, label=\"__class__\", lr=1.0, epoch=75, loss='ova', wordNgrams=2, dim=200, thread=2, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(1,6):\n",
    "#     results = model.test(test_file,k=k)\n",
    "#     print(f\"Test Samples: {results[0]} Precision@{k} : {results[1]*100:2.4f} Recall@{k} : {results[2]*100:2.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained FastText vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import gensim\n",
    "\n",
    "# Download the pre-trained FastText vectors for Latin\n",
    "# You can find other languages here: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "latin_vectors_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.vec.gz'\n",
    "latin_vectors_path = 'models/cc.la.300.vec.gz'\n",
    "\n",
    "# Download the vectors (if not already downloaded)\n",
    "import requests\n",
    "if not os.path.exists(latin_vectors_path):\n",
    "    with requests.get(latin_vectors_url, stream=True) as r:\n",
    "        with open(latin_vectors_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "# Load the vectors using Gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(latin_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'pater':\n",
      "pa-ter: 0.5668\n",
      "avus: 0.5605\n",
      "tahtll: 0.5577\n",
      "filius: 0.5291\n",
      "Épater: 0.4947\n",
      "paterque: 0.4828\n",
      "Proavus: 0.4797\n",
      "mater: 0.4751\n",
      "SENEX: 0.4745\n",
      "avusque: 0.4713\n"
     ]
    }
   ],
   "source": [
    "# Function to find nearest neighbors\n",
    "def get_nearest_neighbors(word, model, top_n=10):\n",
    "    try:\n",
    "        neighbors = model.most_similar(word, topn=top_n)\n",
    "        return neighbors\n",
    "    except KeyError:\n",
    "        return f\"The word '{word}' is not in the vocabulary.\"\n",
    "\n",
    "# Example usage\n",
    "word = 'pater'  # Replace with any Latin word\n",
    "neighbors = get_nearest_neighbors(word, model)\n",
    "\n",
    "print(f\"Nearest neighbors for '{word}':\")\n",
    "for neighbor, similarity in neighbors:\n",
    "    print(f\"{neighbor}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the FastText vectors for Latin if not already downloaded\n",
    "latin_model_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.bin.gz'\n",
    "latin_model_path = 'models/cc.la.300.bin.gz'\n",
    "uncompressed_model_path = 'models/cc.la.300.bin'\n",
    "\n",
    "# do the download with urllib\n",
    "\n",
    "if not os.path.exists(latin_model_path):\n",
    "    urllib.request.urlretrieve(latin_model_url, latin_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the model if not already done\n",
    "if not os.path.exists(uncompressed_model_path):\n",
    "    import gzip\n",
    "    with gzip.open(latin_model_path, 'rb') as f_in:\n",
    "        with open(uncompressed_model_path, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = fasttext.load_model(uncompressed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subwords for 'amaverunt':\n",
      "amaverunt\n",
      "<amav\n",
      "amave\n",
      "maver\n",
      "averu\n",
      "verun\n",
      "erunt\n",
      "runt>\n",
      "\n",
      "Subword vectors for 'amaverunt':\n",
      "Subword: amaverunt -> Vector: [-0.12557602 -0.20677628  0.01814291  0.10018203 -0.03906197 -0.02906827\n",
      "  0.03644218 -0.05640594  0.05151154 -0.01040968]...\n",
      "Subword: <amav -> Vector: [-0.02181956 -0.06025735 -0.08099414 -0.00137296 -0.10970218 -0.08324998\n",
      " -0.00574256  0.05039195 -0.03895114 -0.08521391]...\n",
      "Subword: amave -> Vector: [-0.02245609 -0.02576638 -0.03162137  0.01544662 -0.00128898 -0.05441771\n",
      "  0.01074362  0.01723444 -0.05633336 -0.06492849]...\n",
      "Subword: maver -> Vector: [-0.08401632  0.02202878 -0.03314929  0.0347473  -0.0603159  -0.07957301\n",
      "  0.01893586  0.00751092 -0.06143296 -0.07802811]...\n",
      "Subword: averu -> Vector: [ 0.05511348 -0.02324122 -0.06766613  0.03605975  0.01640362 -0.03939509\n",
      " -0.01637885 -0.04223433 -0.00637189 -0.10575694]...\n",
      "Subword: verun -> Vector: [-0.00134282  0.01997117  0.04394346 -0.08678149  0.08096965 -0.05902625\n",
      " -0.04757813 -0.07660703 -0.12823614  0.06317466]...\n",
      "Subword: erunt -> Vector: [ 0.04660495 -0.02330565  0.00031592 -0.04158193  0.01751084 -0.01030427\n",
      "  0.02243485 -0.00246023  0.02000258 -0.04057171]...\n",
      "Subword: runt> -> Vector: [-0.01892825 -0.03230768  0.00987294 -0.00396597  0.21771072 -0.12216865\n",
      " -0.03965593  0.04795166  0.04763035 -0.05535679]...\n"
     ]
    }
   ],
   "source": [
    "# Function to get subword vectors\n",
    "def get_subword_vectors(word, model):\n",
    "    subwords, indices = model.get_subwords(word)\n",
    "    subword_vectors = model.get_input_matrix()[indices]\n",
    "    return subwords, subword_vectors\n",
    "\n",
    "# Example usage\n",
    "word = 'amaverunt'\n",
    "subwords, subword_vectors = get_subword_vectors(word, model)\n",
    "\n",
    "print(f\"Subwords for '{word}':\")\n",
    "for subword in subwords:\n",
    "    print(subword)\n",
    "\n",
    "print(f\"\\nSubword vectors for '{word}':\")\n",
    "for i, vec in enumerate(subword_vectors):\n",
    "    print(f\"Subword: {subwords[i]} -> Vector: {vec[:10]}...\")  # Printing first 10 dimensions for brevity"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_OnePipeline_ManyClassifiers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
